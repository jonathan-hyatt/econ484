{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"collapsed_sections":["pRi_NHFHBMbT","Hy2j0806315i","qoHuWuUnB9LI","RIzL26GBDavL"],"authorship_tag":"ABX9TyO3SnneSmv6on6rf3NqZD7L"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"a_J_iK_BmyTB"},"source":["# Using Machine Learning to Predict Heterogeneous Treatment Effects"]},{"cell_type":"markdown","metadata":{"id":"G9WBzU6Xn8Ux"},"source":["## Algorithms tailored for predicting outcomes can do poorly when predicting treatment effects"]},{"cell_type":"markdown","metadata":{"id":"hwTTUnLrowli"},"source":["### Factors that strongly predict outcomes may not strongly predict treatment effects"]},{"cell_type":"markdown","metadata":{"id":"THdqEG_Lo4wu"},"source":["$Y_i$: spending on a Lexus\n","\n","$W_i$: seeing an online ad for a Lexus\n","\n","$\\ln Y_i=\\beta_0+\\beta_1 age_i +\\beta_2 male_i + \\beta_3 W_i+\\beta_4 W_i \\times male_i +\\varepsilon_i$\n","\n","How do outcomes vary by age? (A lot if $\\beta_1$ is big)\n","\n","How do treatment effects vary by age? (not at all!)\n","\n","What do treatment effects vary by? (gender!)"]},{"cell_type":"code","metadata":{"id":"W94H6chGvjcm"},"source":["# Install econml\n","!pip install econml"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"5H9c7Kgjtei1"},"source":["# import useful packages\n","import numpy as np\n","import pandas as pd\n","from sklearn.linear_model import LassoCV\n","from sklearn.tree import DecisionTreeRegressor\n","from sklearn import tree\n","from sklearn.model_selection import train_test_split\n","from econml.dml import CausalForestDML as CausalForest\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","sns.set_theme(style=\"whitegrid\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"tdDI1IwMqp5l"},"source":["# define parameters\n","n = 1000 # sample size\n","p = .5 # probability of seeing the ad\n","beta0=0\n","beta1=.2 # effect of age\n","beta2=-.025 # difference in average spending between males and females who don't see the ad ()\n","beta3=0 # effect of treatment among females\n","beta4=.05 # differential effect of treatment among males compared to females\n","sigeps=.02 # residual standard deviation of outcome\n","\n","# generate some fake data\n","age=np.random.randint(low=18,high=61,size=(n,1))\n","male=np.random.randint(low=0,high=2,size=(n,1))\n","w=np.random.rand(n,1)>(1-p)\n","epsilon=sigeps*np.random.randn(n,1)\n","lny=beta0+beta1*age+beta2*male+beta3*w+beta4*w*male+epsilon\n","\n","# assemble as dataframe\n","fakedata=pd.DataFrame(np.concatenate((lny,w,age,male),axis=1),columns=['lny','w','age','male'])\n","fakedata.feature_names=['age','male']\n","\n","# fit trees\n","tree1 = DecisionTreeRegressor(max_depth=2)\n","tree1.fit(fakedata.loc[w==1,['age','male']],fakedata.loc[w==1,['lny']])\n","tree0 = DecisionTreeRegressor(max_depth=2)\n","tree0.fit(fakedata.loc[w==0,['age','male']],fakedata.loc[w==0,['lny']])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"jRK6h_LIG7Ua"},"source":["# look at trees\n","print('Treated tree:')\n","d=tree.plot_tree(tree1,filled=True,feature_names=fakedata.feature_names)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"HZSKYka-G_d1"},"source":["print('Untreated tree:')\n","d=tree.plot_tree(tree0,filled=True,feature_names=fakedata.feature_names)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"GKQp3RqhU3ns"},"source":["Note that both trees used only age. As a result, the predicted treatment effects will not vary at all by gender, which in reality is the only feature upon which treatment effects depend.\n","\n"]},{"cell_type":"markdown","metadata":{"id":"HWglqGJrBn7L"},"source":["### Non-Honest trees can exaggerate differences between groups"]},{"cell_type":"code","metadata":{"id":"gUcZF6a8ByNT"},"source":["#generate a bunch of predictors:\n","x1=np.random.randint(low=0,high=2,size=(n,1))\n","x2=np.random.randint(low=0,high=2,size=(n,1))\n","x3=np.random.randint(low=0,high=2,size=(n,1))\n","x4=np.random.randint(low=0,high=2,size=(n,1))\n","x5=np.random.randint(low=0,high=2,size=(n,1))\n","x6=np.random.randint(low=0,high=2,size=(n,1))\n","x7=np.random.randint(low=0,high=2,size=(n,1))\n","x8=np.random.randint(low=0,high=2,size=(n,1))\n","x9=np.random.randint(low=0,high=2,size=(n,1))\n","x10=np.random.randint(low=0,high=2,size=(n,1))\n","\n","# Each of these predictors affects the outcome with coefficient = 1:\n","y=x1+x2+x3+x4+x5+x6+x7+x8+x9+x10+epsilon\n","\n","# Now grow a tree:\n","predictors=np.concatenate((x1,x2,x3,x4,x5,x6,x7,x8,x9,x10),axis=1)\n","predictor_names=['x1','x2','x3','x4','x5','x6','x7','x8','x9','x10']\n","dishonest=DecisionTreeRegressor(max_depth=2)\n","dishonest.fit(predictors,y)\n","d=tree.plot_tree(dishonest,filled=True,feature_names=predictor_names)\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"RKyYJql9Lec-"},"source":["According to our \"model\" for y, what should be the difference between outcomes in neighboring leaves from the same node? (one!) What are the observed differences in the tree? (greater than one!)\n","\n","Solution: Honest trees, or double-sample trees, use a subset of the training set to grow the tree, and another subset to estimate means in each leaf"]},{"cell_type":"code","metadata":{"id":"PHA5c4xKLmhf"},"source":["train=np.random.randint(low=0,high=2,size=n)==1\n","\n","# Grow tree on training set:\n","dt = DecisionTreeRegressor(max_depth=2)\n","dt.fit(predictors[train,:],y[train])\n","d=tree.plot_tree(dt,filled=True,feature_names=predictor_names)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"yehXF5JIOSOJ"},"source":["# But use estimation set to calculate means in each leaf:\n","leaves=dt.apply(predictors[~train])\n","yest=y[~train]\n","honest=dt\n","n_nodes = honest.tree_.node_count\n","for ii in range(n_nodes):\n","  if honest.tree_.children_left[ii]==-1: # means that the current node is a leaf\n","    estii=yest[leaves==ii]\n","    honest.tree_.value[ii]=estii.mean() # replace the leaf's value with the estimation set's mean\n","\n","d=tree.plot_tree(honest,filled=True,feature_names=predictor_names)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"_odKygGk4j9A"},"source":["The leaf means are not nearly as extreme as in the \"dishonest\" tree that uses the training observations to construct the predictions"]},{"cell_type":"markdown","metadata":{"id":"_ImW8AlReB-f"},"source":["## Random Causal Forest"]},{"cell_type":"code","metadata":{"id":"5OCVpZR7tA-V"},"source":["train, test = train_test_split(fakedata, test_size=0.2)\n","estimator = CausalForest(n_estimators=100)\n","estimator.fit(train['lny'],\n","              train['w'],\n","              X=train[fakedata.feature_names])\n","effects_train = estimator.effect(train[fakedata.feature_names])\n","effects_test = estimator.effect(test[fakedata.feature_names])\n","conf_intrvl = estimator.effect_interval(test[fakedata.feature_names])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"mqoiopLL4v8J"},"source":["malefx_train=effects_train[train['male'].values==1]\n","maleage_train=train['age'].iloc[train['male'].values==1]\n","malefx_train.mean()\n","\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Uai_wDHf428w"},"source":["femalefx_train=effects_train[train['male'].values==0]\n","femaleage_train=train['age'].iloc[train['male'].values==0]\n","femalefx_train.mean()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"XNU90H6A56qv"},"source":["malefx_test=effects_test[test['male'].values==1]\n","malefx_test.mean()"],"execution_count":null,"outputs":[]},{"cell_type":"code","source":["femalefx_test=effects_test[test['male'].values==0]\n","femalefx_test.mean()"],"metadata":{"id":"9on1IUXRp_v-"},"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"MPDiUMH17KDN"},"source":["fig = plt.figure()\n","ax = plt.axes()\n","ax.scatter(maleage_train,malefx_train,label='males');\n","ax.scatter(femaleage_train,femalefx_train,label='females');\n","ax.legend()\n","plt.title(\"Estimated Treatment effects\")\n","plt.xlabel(\"age\")\n","plt.ylabel(\"treatment effect\");"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"YY6X9FMo9nzN"},"source":["Clearly doesn't nail the age profile (which should be flat) but does get the difference between men and women!"]},{"cell_type":"markdown","metadata":{"id":"m1Qj_chk_RxA"},"source":["## Random Causal Forest: Predict the effects of job training\n"]},{"cell_type":"markdown","metadata":{"id":"0Gu4yXtj_p1_"},"source":["We are ready to apply machine learning to predict causal effects in a real-life setting: how do the effects of job training vary by an individual's characteristics? We will use data from the National Job Training Partnership study, a large-scale randomized evaluation of a publicly subsidized job training program for disadvantaged youth and young adults. Why would we care how the effects of a subsidized job training program vary by a person's characteristics?\n"]},{"cell_type":"markdown","metadata":{"id":"d6A5eLn4l9qo"},"source":["We will use the JTPA evaluation dataset, which contains observations on about 14,000 individuals, some of whom were randomized to participate in job training ($z_i = 1$) and others who were not ($z_i = 0$).\n","\n","To do on your own:\n","\n","- load the dataset from the url `https://github.com/Mixtape-Sessions/Heterogeneous-Effects/raw/main/Labs/data/jtpahet.csv`\n","- define the outcome vector (call it `y`) to be the column labeled `foundjob`\n","- define the randomized assignment indicator (call it `z`) to be the column labeled `z`\n","- define the feature vector (call it `x`) to be all columns except `foundjob`, `z`, and `enroll`.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ZxlaYZBvBL_K"},"outputs":[],"source":["# load the data\n","\n","# define the variables"]},{"cell_type":"markdown","metadata":{"id":"pRi_NHFHBMbT"},"source":["### Cheat\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jkgR0g2Yl9qo"},"outputs":[],"source":["data = pd.read_csv(\n","    \"https://github.com/Mixtape-Sessions/Heterogeneous-Effects/raw/main/Labs/data/jtpahet.csv\"\n",")\n","data"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4lvqN0FkmYZc"},"outputs":[],"source":["y = data[\"foundjob\"]\n","z = data[\"z\"]\n","x = data.drop([\"foundjob\", \"z\", \"enroll\"], axis=1)\n","x"]},{"cell_type":"markdown","metadata":{"id":"mrQYHqKWtp-a"},"source":["### Regression to get average effect\n"]},{"cell_type":"markdown","metadata":{"id":"TUD92j-Y3nRE"},"source":["On your own: run a linear regression of the outcome on the random assignment indicator, `z`. Since this was a randomized experiment, we don't need controls!\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0jqgYqcx3yEB"},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{"id":"Hy2j0806315i"},"source":["### Cheat:\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"lOC7Ev8t34Dq"},"outputs":[],"source":["import statsmodels.api as sm\n","rhs = sm.add_constant(\n","    data[\"z\"]\n",")  # you have to add the constant yourself with statsmodels!\n","model = sm.OLS(data[\"foundjob\"], rhs)\n","results = model.fit(cov_type=\"HC3\")  # heteroskedasticity-robust\n","print(results.summary())"]},{"cell_type":"markdown","metadata":{"id":"62yrByRIBk8Z"},"source":["### Set up random forest\n"]},{"cell_type":"markdown","metadata":{"id":"W4fwaxUtBqgN"},"source":["So far, so good? Now create a random causal forest object, and fit it with outcome `y`, treatment variable `z`, and feature matrix `x`.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"OoPlfQhdB459"},"outputs":[],"source":["# On your own: create and fit random causal forest object"]},{"cell_type":"markdown","metadata":{"id":"qoHuWuUnB9LI"},"source":["### Cheat\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"l3UWboTmm2dn"},"outputs":[],"source":["rcf = CausalForest(n_estimators=500, discrete_treatment=True, criterion=\"het\").fit(\n","    y, z, X=x\n",")"]},{"cell_type":"markdown","metadata":{"id":"WniVmeMYCAS9"},"source":["### Explore effects\n"]},{"cell_type":"markdown","metadata":{"id":"EpCYhIH6CHQ1"},"source":["Let's see what kind of heterogeneous effects our random causal forest predicted\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zXZ-maDVnjcf"},"outputs":[],"source":["# calculate the predicted effects:\n","insamplefx = rcf.effect(x)\n","# plot a histogram of the estimated effects, with average effect overlaid\n","fig = plt.figure()\n","ax = plt.axes()\n","ax.hist(insamplefx, bins=30, density=True)\n","plt.axvline(rcf.ate_, color=\"k\", linestyle=\"dashed\", linewidth=1)\n","plt.suptitle(\"Estimated Treatment effects\")\n","plt.title(\"ATE: {:.3g}\".format(rcf.ate_[0]))\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"ysSaIrbpCjgx"},"source":["Let's visualize how these effects vary by prior earnings and education by making a heatmap\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"S7KqJyWaqgZJ"},"outputs":[],"source":["import itertools"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"PS6_MnlavsFn"},"outputs":[],"source":["# create a grid of values for education and prior earnings:\n","educgrid = np.arange(data[\"educ\"].values.min(), data[\"educ\"].values.max() + 1)\n","earngrid = np.arange(\n","    data[\"priorearn\"].values.min(), data[\"priorearn\"].values.max(), 5000\n",")\n","grid = pd.DataFrame(\n","    itertools.product(educgrid, earngrid), columns=[\"educ\", \"priorearn\"]\n",")"]},{"cell_type":"markdown","metadata":{"id":"3dOzvR9AC277"},"source":["We'll first visualize the effects among married, nonwhite females of average age:\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"alNQLH2WuzWU"},"outputs":[],"source":["grid[\"age\"] = data[\"age\"].values.mean()  # set age to the average\n","grid[\"female\"] = 1  # set female = 1\n","grid[\"nonwhite\"] = 1  # set nonwhite = 1\n","grid[\"married\"] = 1  # set married = 1\n","# need to re-order the columns to match x:\n","grid=grid[x.columns]\n","grid"]},{"cell_type":"markdown","metadata":{"id":"hdQVCJVMDPJi"},"source":["To do on your own: calculate the predicted effects for each \"observation\" in the grid:\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rpJtU9CMDaBG"},"outputs":[],"source":["# gridfx = # uncomment and fill in on your own!"]},{"cell_type":"markdown","metadata":{"id":"RIzL26GBDavL"},"source":["### Cheat\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"lSqd6njlz3HI"},"outputs":[],"source":["gridfx = rcf.effect(grid)"]},{"cell_type":"markdown","metadata":{"id":"_oA4yTCCDmGL"},"source":["### Visualize effects with a heatmap:\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"kR7rKsiqokah"},"outputs":[],"source":["from mpl_toolkits.axes_grid1 import make_axes_locatable\n","\n","fig = plt.figure()\n","ax = plt.subplot()\n","main = ax.scatter(\n","    grid[\"educ\"], grid[\"priorearn\"], c=gridfx, cmap=\"plasma\", marker=\"s\", s=300\n",")\n","plt.suptitle(\"Estimated Treatment effects\")\n","plt.title(\"Nonwhite married females\")\n","plt.xlabel(\"years of education\")\n","plt.ylabel(\"prior earnings\")\n","\n","# create an Axes on the right side of ax. The width of cax will be 5%\n","# of ax and the padding between cax and ax will be fixed at 0.05 inch.\n","divider = make_axes_locatable(ax)\n","cax = divider.append_axes(\"right\", size=\"5%\", pad=0.05)\n","plt.colorbar(main, cax=cax)\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"XM9p3yq0DxXP"},"source":["To do on your own: make similar visualizations for males, singles, whites, different ages, etc.\n"]},{"cell_type":"code","metadata":{"id":"uA8jeMVrYyNM"},"source":[],"execution_count":null,"outputs":[]}]}